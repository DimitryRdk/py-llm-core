{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90defeff-9e73-4d94-9da7-a5e822ee9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize an article using the Chain of Density Prompting technique\n",
    "# Article from: https://advanced-stack.com/resources/how-to-summarize-using-chain-of-density-prompting.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d56c3a-0e68-4133-8057-e89ce22daff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e3a832f-ebb7-44c8-a49b-3490b3cfdbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import PyPDF2\n",
    "from llm_core.splitters import TokenSplitter\n",
    "\n",
    "# Open the PDF file\n",
    "with open('data/A Path Towards Autonomous Machine Intelligence.pdf', 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Extract the text from the PDF\n",
    "    pages = []\n",
    "    for page in pdf_reader.pages:\n",
    "        pages.append(page.extract_text())\n",
    "\n",
    "    text = ''.join(pages)\n",
    "\n",
    "def cleanup_unicode(text):\n",
    "    corrected_chars = []\n",
    "    for char in text:\n",
    "        corrected_char = unicodedata.normalize(\"NFKC\", char)\n",
    "        corrected_chars.append(corrected_char)\n",
    "    return \"\".join(corrected_chars)\n",
    "\n",
    "\n",
    "text = cleanup_unicode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1014db5f-fcf1-4155-8884-ecc857f06e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Path Towards Autonomous Machine Intelligence\n",
      "Version 0.9.2, 2022-06-27\n",
      "Yann LeCun\n",
      "Courant Institute of Mathematical Sciences, New York University yann@cs.nyu.edu\n",
      "Meta - Fundamental AI Research yann@fb.com\n",
      "June 27, 2022\n",
      "Abstract\n",
      "How could machines learn as e\u000eciently as humans and animals? How could ma-\n",
      "chines learn to reason and plan? How could machines learn representations of percepts\n",
      "and action plans at multiple levels of abstraction, enabling them to reason, predict,\n",
      "and plan at multiple time horizons? This position paper proposes an architecture and\n",
      "training paradigms with which to construct autonomous intelligent agents. It combines\n",
      "concepts such as con\f",
      "gurable predictive world model, behavior driven through intrinsic\n",
      "motivation, and hierarchical joint embedding architectures trained with self-supervised\n",
      "learning.\n",
      "Keywords: Arti\f",
      "cial Intelligence, Machine Common Sense, Cognitive Architecture, Deep\n",
      "Learning, Self-Supervised Learning, Energy-Based Model, World Models, Joint Embedd\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2937d38-a8cc-4ecd-87bc-5a81528e676f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = TokenSplitter(chunk_size=3_000, chunk_overlap=0)\n",
    "chunks = list(splitter.chunkify(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d295087-c0a4-41be-a5f6-b4e7f4e670d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from llm_core.assistants import OpenAIAssistant, LLaMACPPAssistant\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DenseSummary:\n",
    "    denser_summary: str\n",
    "    missing_entities: list[str]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DenserSummaryCollection:\n",
    "  system_prompt = \"\"\"\n",
    "  You are an expert in writing rich and dense summaries in broad domains.\n",
    "  \"\"\"\n",
    "\n",
    "  prompt = \"\"\"\n",
    "  Article:\n",
    "  {article}\n",
    "  ----\n",
    "\n",
    "  You will generate increasingly concise, entity-dense summaries of the\n",
    "  above Article.\n",
    "\n",
    "  Repeat the following 2 steps 5 times.\n",
    "\n",
    "  - Step 1: Identify 1-3 informative Entities from the Article\n",
    "  which are missing from the previously generated summary and are the most\n",
    "  relevant.\n",
    "\n",
    "  - Step 2: Write a new, denser summary of identical length which covers\n",
    "  every entity and detail from the previous summary plus the missing\n",
    "  entities.\n",
    "\n",
    "  A Missing Entity is:\n",
    "\n",
    "  - Relevant: to the main story\n",
    "  - Specific: descriptive yet concise (5 words or fewer)\n",
    "  - Novel: not in the previous summary\n",
    "  - Faithful: present in the Article\n",
    "  - Anywhere: located anywhere in the Article\n",
    "\n",
    "  Guidelines:\n",
    "  - The first summary should be long (4-5 sentences, approx. 80 words) yet\n",
    "  highly non-specific, containing little information beyond the entities\n",
    "  marked as missing.\n",
    "\n",
    "  - Use overly verbose language and fillers (e.g. \"this article discusses\")\n",
    "  to reach approx. 80 words.\n",
    "\n",
    "  - Make every word count: re-write the previous summary to improve flow and\n",
    "  make space for additional entities.\n",
    "\n",
    "  - Make space with fusion, compression, and removal of uninformative\n",
    "  phrases like \"the article discusses\"\n",
    "\n",
    "  - The summaries should become highly dense and concise yet\n",
    "  self-contained, e.g., easily understood without the Article.\n",
    "\n",
    "  - Missing entities can appear anywhere in the new summary.\n",
    "\n",
    "  - Never drop entities from the previous summary. If space cannot be made,\n",
    "  add fewer new entities.\n",
    "\n",
    "  > Remember to use the exact same number of words for each summary.\n",
    "  Answer in JSON.\n",
    "\n",
    "  > The JSON in `summaries_per_step` should be a list (length 5) of\n",
    "  dictionaries whose keys are \"missing_entities\" and \"denser_summary\".\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  summaries: list[DenseSummary]\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def summarize(cls, article):\n",
    "      with OpenAIAssistant(cls, model='gpt-4') as assistant:\n",
    "          return assistant.process(article=article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c347b02-3721-4785-84bb-7ea373d0ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chunk = chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c59544-0452-4684-a7e3-a256beff1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_collection = DenserSummaryCollection.summarize(first_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e33a5b2-780e-442d-b235-0c4af84bc80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(summary_collection.summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc357fb-1d17-4f9d-9e69-2849f5ec6dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['configurable predictive world models', 'intrinsic motivation', 'hierarchical joint embedding architectures']\n"
     ]
    }
   ],
   "source": [
    "print(summary_collection.summaries[0].missing_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1616694f-9be6-43a7-965f-fba5f5017100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses the author's vision for creating autonomous intelligent agents that learn more like humans and animals. The author proposes an architecture that combines concepts such as configurable predictive world models, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning. The author also identifies three main challenges that AI research must address today. The article is written in a way that appeals to readers from various backgrounds.\n"
     ]
    }
   ],
   "source": [
    "print(summary_collection.summaries[0].denser_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860ec00e-2ec7-4b24-8a4d-fe6ee9762add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_pass_summary_generator(chunks):\n",
    "  for chunk in chunks:\n",
    "    chunk_summaries = DenserSummaryCollection.summarize(chunk)\n",
    "    yield chunk_summaries.summaries[4].denser_summary\n",
    "\n",
    "\n",
    "one_pass_summary = '\\n'.join(single_pass_summary_generator(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3fd5c14-3791-43d7-b0b7-39a4b61c2465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.compute_token_count(one_pass_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbb56764-f54e-49ed-9df0-da90d6b240f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper proposes a vision for autonomous agents, integrating predictive world models, intrinsic motivation, hierarchical joint embedding architectures, including JEPA-1 and JEPA-2, Energy-Based Models (EBMs), Joint Embedding Architectures (JEAs), and Hierarchical JEPA. It scrutinizes a Model-Predictive Control process, emphasizing optimization strategies, and advocates for a multi-scale world model. It investigates machine learning, emphasizing architectural diagrams, and discusses the limitations of Large Language Models (LLMs) and reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "summary_collection = DenserSummaryCollection.summarize(one_pass_summary)\n",
    "final_summary = summary_collection.summaries[4].denser_summary\n",
    "\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60325b9-5f7d-48b8-9571-d1bd4de7005e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bd55e-32a0-4b3c-88aa-73c509036f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using llama 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bef2b09e-9efe-43b7-b395-9c470ca0997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DenserSummaryCollection:\n",
    "  system_prompt = \"\"\"\n",
    "  You are an expert in writing rich and dense summaries in broad domains.\n",
    "  \"\"\"\n",
    "\n",
    "  prompt = \"\"\"\n",
    "  Article:\n",
    "  {article}\n",
    "  ----\n",
    "\n",
    "  You will generate increasingly concise, entity-dense summaries of the\n",
    "  above Article.\n",
    "\n",
    "  Repeat the following 2 steps 5 times.\n",
    "\n",
    "  - Step 1: Identify 1-3 informative Entities from the Article\n",
    "  which are missing from the previously generated summary and are the most\n",
    "  relevant.\n",
    "\n",
    "  - Step 2: Write a new, denser summary of identical length which covers\n",
    "  every entity and detail from the previous summary plus the missing\n",
    "  entities.\n",
    "\n",
    "  A Missing Entity is:\n",
    "\n",
    "  - Relevant: to the main story\n",
    "  - Specific: descriptive yet concise (5 words or fewer)\n",
    "  - Novel: not in the previous summary\n",
    "  - Faithful: present in the Article\n",
    "  - Anywhere: located anywhere in the Article\n",
    "\n",
    "  Guidelines:\n",
    "  - The first summary should be long (4-5 sentences, approx. 80 words) yet\n",
    "  highly non-specific, containing little information beyond the entities\n",
    "  marked as missing.\n",
    "\n",
    "  - Use overly verbose language and fillers (e.g. \"this article discusses\")\n",
    "  to reach approx. 80 words.\n",
    "\n",
    "  - Make every word count: re-write the previous summary to improve flow and\n",
    "  make space for additional entities.\n",
    "\n",
    "  - Make space with fusion, compression, and removal of uninformative\n",
    "  phrases like \"the article discusses\"\n",
    "\n",
    "  - The summaries should become highly dense and concise yet\n",
    "  self-contained, e.g., easily understood without the Article.\n",
    "\n",
    "  - Missing entities can appear anywhere in the new summary.\n",
    "\n",
    "  - Never drop entities from the previous summary. If space cannot be made,\n",
    "  add fewer new entities.\n",
    "\n",
    "  > Remember to use the exact same number of words for each summary.\n",
    "  Answer in JSON.\n",
    "\n",
    "  > The JSON in `summaries_per_step` should be a list (length 5) of\n",
    "  dictionaries whose keys are \"missing_entities\" and \"denser_summary\".\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  summaries: list[DenseSummary]\n",
    "\n",
    "\n",
    "  @classmethod\n",
    "  def summarize(cls, article):\n",
    "      with LLaMACPPAssistant(cls, model='llama-3-q4') as assistant:\n",
    "          return assistant.process(article=article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497c473-729c-4ef0-9f90-7d024487eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pass_summary = '\\n'.join(single_pass_summary_generator(chunks))\n",
    "print(one_pass_summary)\n",
    "\n",
    "summary_collection = DenserSummaryCollection.summarize(one_pass_summary)\n",
    "final_summary = summary_collection.summaries[4].denser_summary\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cc916b0-693d-4b55-b648-4ec11f65422c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An autonomous machine intelligence architecture is proposed combining predictive world models with hierarchical joint embedding architectures. JEPA combines perception, prediction, and action modules for hierarchical planning. The system learns from observation and predicts future outcomes using energy-based models.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab77bc-c4ba-4c25-9b2a-77bad86621f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
