{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f612a7-fa22-4ad5-8cf8-af87567f2139",
   "metadata": {},
   "source": [
    "# Summarize an article using the Chain of Density Prompting technique\n",
    "\n",
    "Original article on [Advanced Stack - Technical Resources](https://advanced-stack.com/resources/how-to-summarize-using-chain-of-density-prompting.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d84f29-4a3d-4ba1-bd3a-c178deaca42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First do the boring stuff of converting PDF to text\n",
    "# pip3 install PyPDF2\n",
    "\n",
    "import unicodedata\n",
    "import PyPDF2\n",
    "\n",
    "from llm_core.splitters import TokenSplitter\n",
    "\n",
    "# Open the PDF file\n",
    "with open('../assets/a-path-towards-autonomous-machines.pdf', 'rb') as file:\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "    # Extract the text from the PDF\n",
    "    pages = []\n",
    "    for page in pdf_reader.pages:\n",
    "        pages.append(page.extract_text())\n",
    "\n",
    "    text = ''.join(pages)\n",
    "\n",
    "def cleanup_unicode(text):\n",
    "    corrected_chars = []\n",
    "    for char in text:\n",
    "        corrected_char = unicodedata.normalize(\"NFKC\", char)\n",
    "        corrected_chars.append(corrected_char)\n",
    "    return \"\".join(corrected_chars)\n",
    "\n",
    "\n",
    "article = cleanup_unicode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64fa33af-f80c-4b50-89dd-6405fa3d56d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39476"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the length in tokens\n",
    "import codecs\n",
    "\n",
    "len(codecs.encode(article, 'tiktoken'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a4268f-2beb-4418-b277-bb6886e3ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_density_system_prompt = \"You are an expert in writing rich and dense summaries in broad domains.\"\n",
    "chain_of_density_prompt = \"\"\"\n",
    "  Article:\n",
    "  {article}\n",
    "  ----\n",
    "\n",
    "  You will generate increasingly concise, entity-dense summaries of the\n",
    "  above Article.\n",
    "\n",
    "  Repeat the following 2 steps 5 times.\n",
    "\n",
    "  - Step 1: Identify 1-3 informative Entities from the Article\n",
    "  which are missing from the previously generated summary and are the most\n",
    "  relevant.\n",
    "\n",
    "  - Step 2: Write a new, denser summary of identical length which covers\n",
    "  every entity and detail from the previous summary plus the missing\n",
    "  entities.\n",
    "\n",
    "  A Missing Entity is:\n",
    "\n",
    "  - Relevant: to the main story\n",
    "  - Specific: descriptive yet concise (5 words or fewer)\n",
    "  - Novel: not in the previous summary\n",
    "  - Faithful: present in the Article\n",
    "  - Anywhere: located anywhere in the Article\n",
    "\n",
    "  Guidelines:\n",
    "  - The first summary should be long (4-5 sentences, approx. 80 words) yet\n",
    "  highly non-specific, containing little information beyond the entities\n",
    "  marked as missing.\n",
    "\n",
    "  - Use overly verbose language and fillers (e.g. \"this article discusses\")\n",
    "  to reach approx. {length_in_words} words.\n",
    "\n",
    "  - Make every word count: re-write the previous summary to improve flow and\n",
    "  make space for additional entities.\n",
    "\n",
    "  - Make space with fusion, compression, and removal of uninformative\n",
    "  phrases like \"the article discusses\"\n",
    "\n",
    "  - The summaries should become highly dense and concise yet\n",
    "  self-contained, e.g., easily understood without the Article.\n",
    "\n",
    "  - Missing entities can appear anywhere in the new summary.\n",
    "\n",
    "  - Never drop entities from the previous summary. If space cannot be made,\n",
    "  add fewer new entities.\n",
    "\n",
    "  > Remember to use the exact same number of words for each summary.\n",
    "  > Write the missing entities in missing_entities\n",
    "  > Write the summary in denser_summary\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d213faa2-679d-4f6b-a442-f62759fd816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define out target structure:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from llm_core.assistants import OpenAIAssistant, OpenWeightsAssistant\n",
    "\n",
    "@dataclass\n",
    "class DenseSummary:\n",
    "    denser_summary: str\n",
    "    missing_entities: List[str]\n",
    "\n",
    "@dataclass\n",
    "class DenserSummaryCollection:\n",
    "    system_prompt = chain_of_density_system_prompt\n",
    "    prompt = chain_of_density_prompt\n",
    "    \n",
    "    summaries: List[DenseSummary]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e15f9-dbfa-4896-af43-215999201dcc",
   "metadata": {},
   "source": [
    "## Generate the summaries with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6aba4876-e011-498d-87b7-4631fb194ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate iteratively the summaries\n",
    "\n",
    "with OpenAIAssistant(DenserSummaryCollection, model='gpt-4o') as assistant:\n",
    "    collection = assistant.process(article=article, length_in_words=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8dc6c51-8384-4095-bb60-cd972d9074e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yann LeCun', 'self-supervised learning', 'energy-based model']\n",
      "['Meta', 'Courant Institute', 'New York University']\n",
      "['energy', 'critic', 'actor']\n",
      "['gradient-based learning', 'intrinsic cost', 'short-term memory']\n",
      "['']\n"
     ]
    }
   ],
   "source": [
    "for summary in collection.summaries:\n",
    "    print(summary.missing_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a68d93d5-cfa7-476d-ad3c-91a3e45da872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yann LeCun's article, affiliated with Meta and the Courant Institute at New York University, discusses the path towards autonomous machine intelligence, focusing on how machines can learn efficiently like humans and animals. It explores the architecture and training paradigms necessary for constructing intelligent agents. The paper combines concepts such as predictive world models, intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning. The goal is to enable machines to reason, predict, and plan at multiple levels of abstraction and time horizons. Key components include energy, critic, and actor modules, gradient-based learning, intrinsic cost, and short-term memory.\n"
     ]
    }
   ],
   "source": [
    "print(collection.summaries[-1].denser_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a43543e-7d32-404f-a66e-5b84a3129bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yann LeCun's article, affiliated with Meta and the Courant Institute at New York University, discusses the path towards autonomous machine intelligence, focusing on how machines can learn efficiently like humans and animals. It explores the architecture and training paradigms necessary for constructing intelligent agents. The paper combines concepts such as predictive world models, intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning. The goal is to enable machines to reason, predict, and plan at multiple levels of abstraction and time horizons.\n"
     ]
    }
   ],
   "source": [
    "print(collection.summaries[2].denser_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf3337-48d9-4d8d-a9c4-9f6e57d943ef",
   "metadata": {},
   "source": [
    "## Generate the summaries with Llama 3.1 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9f360-d093-4dcb-8c40-48db701fa8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate iteratively the summaries\n",
    "\n",
    "with OpenWeightsAssistant(DenserSummaryCollection, model='llama-8b-3.1-q4', loader_kwargs={\"n_ctx\": 50_000}) as assistant:\n",
    "    collection = assistant.process(article=article, length_in_words=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b541a5-c205-42a9-ae04-0e433a995fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd986409-cddc-433a-b2c1-7e13655039a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67f457-aed8-4d4c-b57d-025cf0cf31c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659aef7-c0eb-4f39-abac-fba769a6f83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1e897-05c3-4b83-917e-d27c118c7087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4e421-dfca-4f51-8155-ea5d6aa507a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4210f50-d183-4af7-8de7-5492c51e2fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f778ee7-92b1-45af-871a-554f7f9e3b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12c5c5-7326-4814-a286-c20e5129e1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c655f-0b4f-418a-9f62-d10aebcede4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efb337-a76e-4d86-bf24-72a82423bd57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
